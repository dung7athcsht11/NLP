{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-28T11:23:20.683917Z",
     "start_time": "2025-12-28T11:23:20.666917Z"
    }
   },
   "source": [
    "# T·ª± ƒë·ªông reload l·∫°i code khi b·∫°n s·ª≠a trong file .py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import pandas as pd # D√πng pandas hi·ªÉn th·ªã b·∫£ng l·ªói cho ƒë·∫πp\n",
    "\n",
    "# Import c√°c h√†m t·ª´ file .py c·ªßa b·∫°n\n",
    "from utils.data_loader import load_conll_data, build_dictionary\n",
    "from utils.metrics import count_correct_words, calculate_metrics\n",
    "from methods.baseline import LongestMatching\n",
    "\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n file (Check l·∫°i t√™n file c·ªßa b·∫°n)\n",
    "TRAIN_PATH = 'data/train.conll'\n",
    "TEST_PATH = 'data/test.conll'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:20:29.651601Z",
     "start_time": "2025-12-28T11:20:29.507569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_sentences = load_conll_data(TRAIN_PATH)\n",
    "test_sentences = load_conll_data(TEST_PATH)\n",
    "\n",
    "print(f\"Train size: {len(train_sentences)} c√¢u\")\n",
    "print(f\"Test size:  {len(test_sentences)} c√¢u\")\n",
    "\n",
    "# X√¢y d·ª±ng t·ª´ ƒëi·ªÉn \n",
    "vocab = build_dictionary(train_sentences)\n",
    "print(f\"K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn: {len(vocab)} t·ª´\")"
   ],
   "id": "415d547a5716f10a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8977 c√¢u\n",
      "Test size:  1020 c√¢u\n",
      "K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn: 12756 t·ª´\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Longest Matching test",
   "id": "a30963258914f526"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:22:22.389432Z",
     "start_time": "2025-12-28T11:22:22.319476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "lm_model = LongestMatching(vocab)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "predicted_sentences = []\n",
    "# Ch·∫°y d·ª± ƒëo√°n\n",
    "for gold_sentence in test_sentences:\n",
    "    input_text = \" \".join(gold_sentence).replace(\"_\", \" \")\n",
    "    \n",
    "    pred = lm_model.segment(input_text)\n",
    "    predicted_sentences.append(pred)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Ho√†n th√†nh trong {end_time - start_time:.2f} gi√¢y.\")"
   ],
   "id": "3d9f760b088321bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ho√†n th√†nh trong 0.05 gi√¢y.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "T√≠nh ƒëi·ªÉm",
   "id": "f015ed8e761485a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:23:23.897376Z",
     "start_time": "2025-12-28T11:23:23.875940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# T√≠nh ƒëi·ªÉm\n",
    "correct, total_pred, total_gold = count_correct_words(predicted_sentences, test_sentences)\n",
    "p, r, f1 = calculate_metrics(correct, total_pred, total_gold)\n",
    "\n",
    "print(\"K·∫æT QU·∫¢: \")\n",
    "print(f\"Precision: {p*100:.2f}%\")\n",
    "print(f\"Recall:    {r*100:.2f}%\")\n",
    "print(f\"F1-Score:  {f1*100:.2f}%\")"
   ],
   "id": "a2b036465ca43c47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫æT QU·∫¢: \n",
      "Precision: 94.09%\n",
      "Recall:    95.21%\n",
      "F1-Score:  94.64%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T11:24:26.004447Z",
     "start_time": "2025-12-28T11:24:25.985031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# T√¨m c√°c c√¢u l√†m sai\n",
    "errors = []\n",
    "for i in range(len(test_sentences)):\n",
    "    if predicted_sentences[i] != test_sentences[i]:\n",
    "        errors.append({\n",
    "            \"Input\": \" \".join(test_sentences[i]).replace(\"_\", \" \"),\n",
    "            \"ƒê√∫ng\": \" \".join(test_sentences[i]),\n",
    "            \"D·ª± ƒëo√°n\": \" \".join(predicted_sentences[i])\n",
    "        })\n",
    "        \n",
    "    if len(errors) >= 10: break \n",
    "\n",
    "df_errors = pd.DataFrame(errors)\n",
    "\n",
    "print(\"DANH S√ÅCH C√ÅC C√ÇU B·ªä T√ÅCH SAI:\")\n",
    "display(df_errors)"
   ],
   "id": "eb88d68fd172eeb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DANH S√ÅCH C√ÅC C√ÇU B·ªä T√ÅCH SAI:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                     Input  \\\n",
       "0                       ƒê√¢y l√† Phan Rang .   \n",
       "1              M·∫•t uy t√≠n v·ªõi ƒë·ªìng b·ªçn ...   \n",
       "2                      ƒêi t·ª´ kh√≥ ngh√®o ...   \n",
       "3          VKSND t·ªânh B·∫øn Tre kh√°ng ngh·ªã .   \n",
       "4                C·∫£ v∆∞·ªùn th√∫ bu·ªìn th∆∞∆°ng .   \n",
       "5                R·ªìi ch√∫ng t√¥i l·∫°i ra ƒëi .   \n",
       "6           Cu·ªôc thi K√Ω s·ª± nh√¢n v·∫≠t 2005 .   \n",
       "7  Tr·∫°i Kem Kjang ... 864 gi·ªù kh·∫Øc kho·∫£i .   \n",
       "8                   C·ª© th·∫ø ƒë√£ g·∫ßn 20 nƒÉm .   \n",
       "9       Ch·ªã ·∫•y r·∫•t ƒë·ªô l∆∞·ª£ng v√† th√¥ng c·∫£m .   \n",
       "\n",
       "                                      ƒê√∫ng  \\\n",
       "0                       ƒê√¢y l√† Phan_Rang .   \n",
       "1              M·∫•t uy_t√≠n v·ªõi ƒë·ªìng_b·ªçn ...   \n",
       "2                      ƒêi t·ª´ kh√≥ ngh√®o ...   \n",
       "3          VKSND t·ªânh B·∫øn_Tre kh√°ng_ngh·ªã .   \n",
       "4                C·∫£ v∆∞·ªùn th√∫ bu·ªìn th∆∞∆°ng .   \n",
       "5                R·ªìi ch√∫ng_t√¥i l·∫°i ra ƒëi .   \n",
       "6           Cu·ªôc thi K√Ω_s·ª± nh√¢n_v·∫≠t 2005 .   \n",
       "7  Tr·∫°i Kem_Kjang ... 864 gi·ªù kh·∫Øc_kho·∫£i .   \n",
       "8                   C·ª© th·∫ø ƒë√£ g·∫ßn 20 nƒÉm .   \n",
       "9       Ch·ªã ·∫•y r·∫•t ƒë·ªô_l∆∞·ª£ng v√† th√¥ng_c·∫£m .   \n",
       "\n",
       "                                   D·ª± ƒëo√°n  \n",
       "0                       ƒê√¢y l√† Phan Rang .  \n",
       "1              M·∫•t uy_t√≠n v·ªõi ƒë·ªìng b·ªçn ...  \n",
       "2                      ƒêi t·ª´ kh√≥_ngh√®o ...  \n",
       "3          VKSND t·ªânh B·∫øn_Tre kh√°ng ngh·ªã .  \n",
       "4                C·∫£ v∆∞·ªùn_th√∫ bu·ªìn th∆∞∆°ng .  \n",
       "5                R·ªìi ch√∫ng_t√¥i l·∫°i ra_ƒëi .  \n",
       "6           Cu·ªôc_thi K√Ω_s·ª± nh√¢n_v·∫≠t 2005 .  \n",
       "7  Tr·∫°i Kem Kjang ... 864 gi·ªù_kh·∫Øc kho·∫£i .  \n",
       "8                   C·ª©_th·∫ø ƒë√£ g·∫ßn 20 nƒÉm .  \n",
       "9       Ch·ªã ·∫•y r·∫•t ƒë·ªô l∆∞·ª£ng v√† th√¥ng_c·∫£m .  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>ƒê√∫ng</th>\n",
       "      <th>D·ª± ƒëo√°n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ƒê√¢y l√† Phan Rang .</td>\n",
       "      <td>ƒê√¢y l√† Phan_Rang .</td>\n",
       "      <td>ƒê√¢y l√† Phan Rang .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M·∫•t uy t√≠n v·ªõi ƒë·ªìng b·ªçn ...</td>\n",
       "      <td>M·∫•t uy_t√≠n v·ªõi ƒë·ªìng_b·ªçn ...</td>\n",
       "      <td>M·∫•t uy_t√≠n v·ªõi ƒë·ªìng b·ªçn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ƒêi t·ª´ kh√≥ ngh√®o ...</td>\n",
       "      <td>ƒêi t·ª´ kh√≥ ngh√®o ...</td>\n",
       "      <td>ƒêi t·ª´ kh√≥_ngh√®o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VKSND t·ªânh B·∫øn Tre kh√°ng ngh·ªã .</td>\n",
       "      <td>VKSND t·ªânh B·∫øn_Tre kh√°ng_ngh·ªã .</td>\n",
       "      <td>VKSND t·ªânh B·∫øn_Tre kh√°ng ngh·ªã .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C·∫£ v∆∞·ªùn th√∫ bu·ªìn th∆∞∆°ng .</td>\n",
       "      <td>C·∫£ v∆∞·ªùn th√∫ bu·ªìn th∆∞∆°ng .</td>\n",
       "      <td>C·∫£ v∆∞·ªùn_th√∫ bu·ªìn th∆∞∆°ng .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R·ªìi ch√∫ng t√¥i l·∫°i ra ƒëi .</td>\n",
       "      <td>R·ªìi ch√∫ng_t√¥i l·∫°i ra ƒëi .</td>\n",
       "      <td>R·ªìi ch√∫ng_t√¥i l·∫°i ra_ƒëi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cu·ªôc thi K√Ω s·ª± nh√¢n v·∫≠t 2005 .</td>\n",
       "      <td>Cu·ªôc thi K√Ω_s·ª± nh√¢n_v·∫≠t 2005 .</td>\n",
       "      <td>Cu·ªôc_thi K√Ω_s·ª± nh√¢n_v·∫≠t 2005 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tr·∫°i Kem Kjang ... 864 gi·ªù kh·∫Øc kho·∫£i .</td>\n",
       "      <td>Tr·∫°i Kem_Kjang ... 864 gi·ªù kh·∫Øc_kho·∫£i .</td>\n",
       "      <td>Tr·∫°i Kem Kjang ... 864 gi·ªù_kh·∫Øc kho·∫£i .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C·ª© th·∫ø ƒë√£ g·∫ßn 20 nƒÉm .</td>\n",
       "      <td>C·ª© th·∫ø ƒë√£ g·∫ßn 20 nƒÉm .</td>\n",
       "      <td>C·ª©_th·∫ø ƒë√£ g·∫ßn 20 nƒÉm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ch·ªã ·∫•y r·∫•t ƒë·ªô l∆∞·ª£ng v√† th√¥ng c·∫£m .</td>\n",
       "      <td>Ch·ªã ·∫•y r·∫•t ƒë·ªô_l∆∞·ª£ng v√† th√¥ng_c·∫£m .</td>\n",
       "      <td>Ch·ªã ·∫•y r·∫•t ƒë·ªô l∆∞·ª£ng v√† th√¥ng_c·∫£m .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:35:17.253015Z",
     "start_time": "2025-12-28T16:35:12.413899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# 1. H√†m r√∫t tr√≠ch ƒë·∫∑c tr∆∞ng (Feature Extraction)\n",
    "# CRF c·∫ßn nh√¨n v√†o \"ng·ªØ c·∫£nh\" xung quanh t·ª´ ƒë·ªÉ quy·∫øt ƒë·ªãnh\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0] # L·∫•y t·ª´ (syllable)\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.isupper()': word.isupper(), # C√≥ vi·∫øt hoa kh√¥ng?\n",
    "        'word.istitle()': word.istitle(), # Vi·∫øt hoa ch·ªØ ƒë·∫ßu?\n",
    "        'word.isdigit()': word.isdigit(), # L√† s·ªë?\n",
    "    }\n",
    "    \n",
    "    # Nh√¨n t·ª´ ph√≠a tr∆∞·ªõc (Previous word)\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True # ƒê·∫ßu c√¢u\n",
    "\n",
    "    # Nh√¨n t·ª´ ph√≠a sau (Next word)\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # Cu·ªëi c√¢u\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "# 2. Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang d·∫°ng B-I-W (Begin - Inside - Word)\n",
    "# V√¨ CRF l√† b√†i to√°n g√°n nh√£n, ta c·∫ßn chuy·ªÉn t√°ch t·ª´ v·ªÅ g√°n nh√£n\n",
    "# V√≠ d·ª•: \"H·ªçc_sinh\" -> [(\"H·ªçc\", \"B_W\"), (\"sinh\", \"I_W\")]\n",
    "def convert_to_bi_labels(sentences):\n",
    "    formatted_data = []\n",
    "    for sent in sentences:\n",
    "        labeled_sent = []\n",
    "        for word in sent:\n",
    "            syllables = word.split('_')\n",
    "            # √Çm ti·∫øt ƒë·∫ßu ti√™n c·ªßa t·ª´ g√°n nh√£n B_W\n",
    "            labeled_sent.append((syllables[0], 'B_W'))\n",
    "            # C√°c √¢m ti·∫øt sau g√°n nh√£n I_W\n",
    "            for s in syllables[1:]:\n",
    "                labeled_sent.append((s, 'I_W'))\n",
    "        formatted_data.append(labeled_sent)\n",
    "    return formatted_data\n",
    "\n",
    "# --- CH·∫†Y TH·ª∞C NGHI·ªÜM ---\n",
    "\n",
    "# A. Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu\n",
    "print(\"ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang format B-I-W...\")\n",
    "train_data_bi = convert_to_bi_labels(train_sentences)\n",
    "test_data_bi = convert_to_bi_labels(test_sentences)\n",
    "\n",
    "# B. T·∫°o Features (X) v√† Labels (y)\n",
    "X_train = [sent2features(s) for s in train_data_bi]\n",
    "y_train = [sent2labels(s) for s in train_data_bi]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_data_bi]\n",
    "y_test = [sent2labels(s) for s in test_data_bi]\n",
    "\n",
    "# C. Hu·∫•n luy·ªán m√¥ h√¨nh (Training)\n",
    "print(\" ƒêang hu·∫•n luy·ªán m√¥ h√¨nh CRF\")\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.046,\n",
    "    c2=0.049,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# D. D·ª± ƒëo√°n v√† ƒê√°nh gi√°\n",
    "print(\" -> ƒêang d·ª± ƒëo√°n tr√™n t·∫≠p Test...\")\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "# H√†m chuy·ªÉn ƒë·ªïi ng∆∞·ª£c t·ª´ nh√£n B-I v·ªÅ c√¢u t√°ch t·ª´ ƒë·ªÉ t√≠nh F1 c≈©\n",
    "def rebuild_sentences(tokens, labels):\n",
    "    rebuilt_sentences = []\n",
    "    for i in range(len(tokens)):\n",
    "        sent_tokens = [t[0] for t in tokens[i]] # L·∫•y danh s√°ch √¢m ti·∫øt\n",
    "        sent_labels = labels[i]\n",
    "        \n",
    "        words = []\n",
    "        current_word = []\n",
    "        for j, (token, label) in enumerate(zip(sent_tokens, sent_labels)):\n",
    "            if label == 'B_W':\n",
    "                if current_word:\n",
    "                    words.append(\"_\".join(current_word))\n",
    "                current_word = [token]\n",
    "            elif label == 'I_W':\n",
    "                current_word.append(token)\n",
    "            else: # Tr∆∞·ªùng h·ª£p nh√£n l·∫°, coi nh∆∞ t·ª´ m·ªõi\n",
    "                if current_word:\n",
    "                    words.append(\"_\".join(current_word))\n",
    "                current_word = [token]\n",
    "        if current_word:\n",
    "            words.append(\"_\".join(current_word))\n",
    "        rebuilt_sentences.append(words)\n",
    "    return rebuilt_sentences\n",
    "\n",
    "# T√°i t·∫°o l·∫°i c√¢u t·ª´ k·∫øt qu·∫£ d·ª± ƒëo√°n c·ªßa CRF\n",
    "pred_sentences_crf = rebuild_sentences(test_data_bi, y_pred)\n",
    "\n",
    "# T√≠nh ƒëi·ªÉm b·∫±ng h√†m c≈© c·ªßa b·∫°n\n",
    "c, tp, tg = count_correct_words(pred_sentences_crf, test_sentences)\n",
    "p, r, f1 = calculate_metrics(c, tp, tg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" K·∫æT QU·∫¢ C·∫¢I TI·∫æN (CRF MODEL)\")\n",
    "print(\"=\"*40)\n",
    "print(f\" Precision: {p*100:.2f}%\")\n",
    "print(f\" Recall:    {r*100:.2f}%\")\n",
    "print(f\" F1-Score:  {f1*100:.2f}%\")\n",
    "print(\"=\"*40)"
   ],
   "id": "5a3085369bbddbae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang format B-I-W...\n",
      " ƒêang hu·∫•n luy·ªán m√¥ h√¨nh CRF\n",
      " -> ƒêang d·ª± ƒëo√°n tr√™n t·∫≠p Test...\n",
      "\n",
      "========================================\n",
      " K·∫æT QU·∫¢ C·∫¢I TI·∫æN (CRF MODEL)\n",
      "========================================\n",
      " Precision: 91.00%\n",
      " Recall:    91.51%\n",
      " F1-Score:  91.26%\n",
      "========================================\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:24:28.476251Z",
     "start_time": "2025-12-28T16:24:22.652809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from methods.CFR import CRFSegmenter # Import t·ª´ file m·ªõi t·∫°o\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.0406,\n",
    "    c2=0.0494,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "print(\"üöÄB·∫ÆT ƒê·∫¶U TH·ª∞C NGHI·ªÜM C·∫¢I TI·∫æN (CRF MODEL)\")\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o v√† Train m√¥ h√¨nh\n",
    "crf_model = CRFSegmenter(dictionary=vocab) \n",
    "\n",
    "crf_model.train(train_sentences) # D√πng l·∫°i bi·∫øn train_sentences ƒë√£ load ·ªü tr√™n\n",
    "\n",
    "# 2. Ch·∫°y th·ª≠ nghi·ªám tr√™n t·∫≠p Test\n",
    "print(\"\\n ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n t·∫≠p Test...\")\n",
    "crf_predictions = []\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for gold in test_sentences:\n",
    "    raw_input = \" \".join(gold).replace(\"_\", \" \")\n",
    "    pred = crf_model.segment(raw_input) # G·ªçi h√†m segment y h·ªát Baseline\n",
    "    crf_predictions.append(pred)\n",
    "end = time.time()\n",
    "print(f\" Ho√†n th√†nh trong {end - start:.2f} gi√¢y.\")\n",
    "\n",
    "# 3. ƒê√°nh gi√° k·∫øt qu·∫£\n",
    "c, tp, tg = count_correct_words(crf_predictions, test_sentences)\n",
    "p, r, f1 = calculate_metrics(c, tp, tg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"K·∫æT QU·∫¢ MODEL C·∫¢I TI·∫æN (CRF)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Precision: {p*100:.2f}%\")\n",
    "print(f\"Recall:    {r*100:.2f}%\")\n",
    "print(f\" F1-Score:  {f1*100:.2f}%\")\n",
    "print(\"=\"*40)"
   ],
   "id": "7a524de08bf7e130",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄB·∫ÆT ƒê·∫¶U TH·ª∞C NGHI·ªÜM C·∫¢I TI·∫æN (CRF MODEL)\n",
      " -> [CRF Hybrid] ƒêang chu·∫©n b·ªã d·ªØ li·ªáu & features...\n",
      " -> [CRF Hybrid] ƒêang training model...\n",
      " -> [CRF Hybrid] Training ho√†n t·∫•t!\n",
      "\n",
      " ƒêang ch·∫°y d·ª± ƒëo√°n tr√™n t·∫≠p Test...\n",
      " Ho√†n th√†nh trong 0.15 gi√¢y.\n",
      "\n",
      "========================================\n",
      "K·∫æT QU·∫¢ MODEL C·∫¢I TI·∫æN (CRF)\n",
      "========================================\n",
      "Precision: 91.54%\n",
      "Recall:    92.09%\n",
      " F1-Score:  91.81%\n",
      "========================================\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:24:31.026406Z",
     "start_time": "2025-12-28T16:24:31.000405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import methods.CFR\n",
    "import importlib\n",
    "\n",
    "# √âp t·∫£i l·∫°i file crf_model.py m·ªõi nh·∫•t t·ª´ ƒëƒ©a c·ª©ng\n",
    "importlib.reload(methods.CFR)\n",
    "\n",
    "# Import l·∫°i class sau khi reload\n",
    "from methods.CFR import CRFSegmenter\n",
    "\n",
    "print(\" ƒê√£ c·∫≠p nh·∫≠t code m·ªõi th√†nh c√¥ng!\")"
   ],
   "id": "4670bd7e4d93578",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ƒê√£ c·∫≠p nh·∫≠t code m·ªõi th√†nh c√¥ng!\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:38:32.052107Z",
     "start_time": "2025-12-28T16:38:02.177085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import metrics\n",
    "from methods.CFR import CRFSegmenter # Import class c·ªßa b·∫°n\n",
    "\n",
    "print(\" ƒêANG CHU·∫®N B·ªä D·ªÆ LI·ªÜU ƒê·ªÇ TUNING...\")\n",
    "\n",
    "segmenter = CRFSegmenter(dictionary=vocab)\n",
    "\n",
    "print(\" -> ƒêang t·∫°o Features cho t·∫≠p Train...\")\n",
    "train_data_bi = segmenter._convert_to_bi_labels(train_sentences)\n",
    "X_train = [segmenter._sent2features(s) for s in train_data_bi]\n",
    "y_train = [segmenter._sent2labels(s) for s in train_data_bi]\n",
    "\n",
    "# 3. ƒê·ªãnh nghƒ©a kh√¥ng gian tham s·ªë c·∫ßn th·ª≠\n",
    "# c1 v√† c2 s·∫Ω ƒë∆∞·ª£c l·∫•y ng·∫´u nhi√™n theo ph√¢n ph·ªëi m≈© (Exponential distribution)\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# 4. Thi·∫øt l·∫≠p m√¥ h√¨nh CRF c∆° b·∫£n\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# S·ª≠ d·ª•ng F1-Score trung b√¨nh ƒë·ªÉ ƒë√°nh gi√°\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted', labels=['B_W', 'I_W'])\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    crf, \n",
    "    params_space, \n",
    "    cv=3, \n",
    "    verbose=1, \n",
    "    n_jobs=-1, \n",
    "    n_iter=10, \n",
    "    scoring=f1_scorer\n",
    ")\n",
    "\n",
    "print(f\"\\nB·∫ÆT ƒê·∫¶U CH·∫†Y T√åM KI·∫æM (S·∫Ω th·ª≠ {10 * 3} l∆∞·ª£t train)...\")\n",
    "try:\n",
    "    rs.fit(X_train, y_train)\n",
    "    \n",
    "    # 6. In k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"K·∫æT QU·∫¢ T·ªêI ∆ØU (BEST PARAMS)\")\n",
    "    print(\"=\"*40)\n",
    "    print('Best params:', rs.best_params_)\n",
    "    print('Best CV score:', rs.best_score_)\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # 7. L∆∞u l·∫°i tham s·ªë t·ªët nh·∫•t ƒë·ªÉ d√πng\n",
    "    best_c1 = rs.best_params_['c1']\n",
    "    best_c2 = rs.best_params_['c2']\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\nC√≥ l·ªói x·∫£y ra:\", e)\n",
    "    # N·∫øu l·ªói, d√πng tham s·ªë m·∫∑c ƒë·ªãnh\n",
    "    best_c1 = 0.1\n",
    "    best_c2 = 0.1\n",
    "\n",
    "print(f\"\\n c1={best_c1:.4f}, c2={best_c2:.4f}\")"
   ],
   "id": "dabced366899dc2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ƒêANG CHU·∫®N B·ªä D·ªÆ LI·ªÜU ƒê·ªÇ TUNING...\n",
      " -> ƒêang t·∫°o Features cho t·∫≠p Train...\n",
      "\n",
      "B·∫ÆT ƒê·∫¶U CH·∫†Y T√åM KI·∫æM (S·∫Ω th·ª≠ 30 l∆∞·ª£t train)...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "========================================\n",
      "K·∫æT QU·∫¢ T·ªêI ∆ØU (BEST PARAMS)\n",
      "========================================\n",
      "Best params: {'c1': np.float64(0.33995133103188707), 'c2': np.float64(0.017641623395822585)}\n",
      "Best CV score: 0.950281044833933\n",
      "========================================\n",
      "\n",
      " c1=0.3400, c2=0.0176\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a25ff863eeb9454"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
